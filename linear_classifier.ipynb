{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "class MyLogReg():\n",
    "    def __init__(self, n_iter, learning_rate, weights=None, metric=None, reg=None, l1_coef=0, l2_coef=0, sgd_sample=None, random_state=42):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.random_state = random_state\n",
    "        self.sgd_sample = sgd_sample\n",
    "\n",
    "        self.metric = self.metrics(metric)\n",
    "        self.learning_rate = self.learning_rate_type(learning_rate)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\"\n",
    "\n",
    "    def calculate_gradient(self, X, y, y_pred):\n",
    "        if self.reg == 'l1':\n",
    "            assert self.l1_coef != 0\n",
    "            return (1/X.shape[0]) * np.dot((y_pred - y.values.ravel()), X) + self.l1_coef*np.sign(self.weights)\n",
    "        elif self.reg == 'l2':\n",
    "            assert self.l2_coef != 0\n",
    "            return (1/X.shape[0]) * np.dot((y_pred - y.values.ravel()), X) + self.l2_coef*2*(self.weights)\n",
    "        elif self.reg == 'elasticnet':\n",
    "            assert self.l1_coef != 0 and self.l2_coef != 0\n",
    "            return (1/X.shape[0]) * np.dot((y_pred - y.values.ravel()), X) + self.l1_coef*np.sign(self.weights) + self.l2_coef*2*(self.weights)\n",
    "        else:\n",
    "            return (1/X.shape[0]) * np.dot((y_pred - y.values.ravel()), X)\n",
    "\n",
    "    def learning_rate_type(self, LR):\n",
    "        if not isinstance(LR, float):\n",
    "            return LR\n",
    "        else:\n",
    "            return lambda x: LR\n",
    "        \n",
    "    def sgd_sample_size(self, sgd_sample_num, samples):\n",
    "        if isinstance(sgd_sample_num, float):\n",
    "            return int(samples.shape[0] * sgd_sample_num)\n",
    "        elif isinstance(sgd_sample_num, int):\n",
    "            return sgd_sample_num\n",
    "        else:\n",
    "            return samples.shape[0]\n",
    "\n",
    "    def metrics(self, metric) -> None:\n",
    "        if metric:\n",
    "            if metric == 'accuracy':\n",
    "                metric = ['accuracy', lambda y, y_pred: (\n",
    "                    np.sum(y.values.ravel() == (y_pred > 0.5).ravel())) / len(y)]\n",
    "\n",
    "            elif metric == 'precision':\n",
    "                # TP / (TP + FP)\n",
    "                def precision_score_func(y, y_pred):\n",
    "                    y_pred_binary = (y_pred > 0.5).ravel()\n",
    "                    y_ravel = y.values.ravel()\n",
    "                    TP = np.sum((y_ravel == 1) & (y_pred_binary == 1))\n",
    "                    FP = np.sum((y_ravel == 0) & (y_pred_binary == 1))\n",
    "                    return TP/(TP+FP)\n",
    "\n",
    "                metric = ['precision', precision_score_func]\n",
    "\n",
    "            elif metric == 'recall':\n",
    "                # TP / (TP + FN)\n",
    "                def recall_score_func(y, y_pred):\n",
    "                    y_pred_binary = (y_pred > 0.5)\n",
    "                    y_ravel = y.values.ravel()\n",
    "                    TP = np.sum((y_ravel == 1) & (y_pred_binary == 1))\n",
    "                    FN = np.sum((y_ravel == 1) & (y_pred_binary == 0))\n",
    "                    return TP/(TP+FN)\n",
    "                \n",
    "                metric = ['recall', recall_score_func]\n",
    "\n",
    "            elif metric == 'f1':\n",
    "                # 2 * precision * recall / (precision + recall)\n",
    "                def f1_score_func(y, y_pred):\n",
    "                    y_pred_binary = (y_pred > 0.5)\n",
    "                    y_ravel = y.values.ravel()\n",
    "                    TP = np.sum((y_ravel == 1) & (y_pred_binary == 1))\n",
    "                    FP = np.sum((y_ravel == 0) & (y_pred_binary == 1))\n",
    "                    FN = np.sum((y_ravel == 1) & (y_pred_binary == 0))\n",
    "                    precision = TP/(TP+FP)\n",
    "                    recall = TP/(TP+FN)\n",
    "                    return 2 * precision * recall / (precision + recall)\n",
    "                \n",
    "                metric = ['f1', f1_score_func]\n",
    "\n",
    "\n",
    "            elif metric == 'roc_auc':\n",
    "                def auc_score_def(y, y_pred):\n",
    "                    data = np.concatenate(\n",
    "                        (y.to_numpy().reshape(-1, 1), np.round(y_pred.reshape(-1, 1), 10)), axis=1)\n",
    "                    data = data[data[:, 1].argsort()][::-1]\n",
    "\n",
    "                    pos_above_iter = 0\n",
    "\n",
    "                    for y, pred in data:\n",
    "                        if y == 0:\n",
    "                            if (data[data[:, 1] == pred]).shape[0] > 1:\n",
    "                                pos_above_iter += np.sum(\n",
    "                                    data[data[:, 1] > pred][:, 0], axis=0) / 2\n",
    "                            pos_above_iter += np.sum(\n",
    "                                data[data[:, 1] > pred][:, 0], axis=0)\n",
    "                    return pos_above_iter / (np.sum(data[:, 0] == 1) * np.sum(data[:, 0] == 0))\n",
    "\n",
    "                metric = ['roc_auc', auc_score_def]\n",
    "\n",
    "        return metric\n",
    "\n",
    "    def fit(self, samples: pd.DataFrame, y: pd.Series, verbose=False) -> None:\n",
    "        random.seed(self.random_state)  # фиксируем рандом сид\n",
    "\n",
    "        sgd_sample_quantity = self.sgd_sample_size(self.sgd_sample, samples)\n",
    "\n",
    "        X = samples.copy()\n",
    "        X.insert(0, 'bias', pd.Series(1, index=X.index))\n",
    "\n",
    "        self.weights = np.ones(X.shape[1])\n",
    "\n",
    "        for iter in range(1, self.n_iter+1):\n",
    "            sample_rows_idx = random.sample(\n",
    "                range(X.shape[0]), sgd_sample_quantity)\n",
    "            X_mini_batch = X.iloc[sample_rows_idx]\n",
    "            y_mini_batch = y.iloc[sample_rows_idx]\n",
    "\n",
    "            y_pred = np.array(1/(1 + np.exp(-np.dot(X_mini_batch, self.weights))))\n",
    "\n",
    "            y_pred_for_loss = np.array(1/(1 + np.exp(-np.dot(X, self.weights))))\n",
    "\n",
    "            loss = -np.mean(np.log(y_pred_for_loss+1e-100)*y.values.ravel() +\n",
    "                            np.log(1 - y_pred_for_loss+1e-100)*(1-y.values.ravel()))\n",
    "\n",
    "            # grad = (1/(X.shape[0]) * np.dot((y_pred - y.values.ravel()), X))\n",
    "            grad = self.calculate_gradient(X_mini_batch, y_mini_batch, y_pred)\n",
    "\n",
    "            self.weights = self.weights - grad * self.learning_rate(iter)\n",
    "            if verbose and (iter % verbose) == 0 and self.metric is not None:\n",
    "                print(\n",
    "                    f'iter = {iter+1} ||| Loss = {loss} ||| {self.metric[0]} = {self.metric[1](y, y_pred_for_loss)}')\n",
    "            elif verbose and (iter % verbose) == 0:\n",
    "                print(f'iter = {iter+1} ||| Loss = {loss}')\n",
    "        if self.metric:\n",
    "            self.final_metric = self.metric[1](y, np.array(\n",
    "                1/(1 + np.exp(-np.dot(X, self.weights)))))\n",
    "\n",
    "    def predict(self, samples):\n",
    "        X = samples.copy()\n",
    "        X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "        return (np.array(1/(1 + np.exp(-np.dot(X, self.weights)))) > 0.5).astype(np.int8)\n",
    "\n",
    "    def predict_proba(self, samples):\n",
    "        X = samples.copy()\n",
    "        X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "        return np.array(1/(1 + np.exp(-np.dot(X, self.weights))))\n",
    "\n",
    "    def get_coef(self) -> list():\n",
    "        try:\n",
    "            assert self.weights is not None\n",
    "            return np.array(self.weights[1:])\n",
    "        except:\n",
    "            return 'fit before!'\n",
    "\n",
    "    def get_best_score(self) -> int:\n",
    "        return self.final_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 11 ||| Loss = 35.631830863566286 ||| precision = 0.643979057591623\n",
      "iter = 21 ||| Loss = 16.181827325897686 ||| precision = 0.6911764705882353\n",
      "iter = 31 ||| Loss = 13.34606526399461 ||| precision = 0.7177033492822966\n",
      "iter = 41 ||| Loss = 12.627028679628506 ||| precision = 0.7142857142857143\n",
      "iter = 51 ||| Loss = 12.579146054146257 ||| precision = 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "x = MyLogReg(50, lambda iter: 0.5 * (0.85 ** iter), metric='precision', sgd_sample=0.1, reg='elasticnet', l1_coef=0.1, l2_coef=0.1)\n",
    "\n",
    "np.random.seed(42)\n",
    "# X_1 = pd.DataFrame({'first':[50 for x in range(200)], 'second':[0 for x in range(200)]})\n",
    "# X_2 = pd.DataFrame({'first':[0 for x in range(200)], 'second':[50 for x in range(200)]})\n",
    "\n",
    "# X = pd.concat((X_1, X_2), axis=0)\n",
    "X = pd.DataFrame(np.random.randint(-25, 25, (400, 100)))\n",
    "y = pd.DataFrame({'target': [0 if x<200 else 1 for x in range(400)]})\n",
    "\n",
    "x.fit(X, y, verbose=10)\n",
    "\n",
    "# np.mean((x.predict_proba(X) > 0.5) == y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.29319297e-02,  1.53071071e-01, -2.40637417e-01, -3.86252539e-01,\n",
       "       -3.17564549e-02, -4.90962676e-03,  2.07070920e-01, -8.29053212e-02,\n",
       "        5.82039659e-02,  3.66321087e-01,  2.61473751e-01, -1.69966116e-01,\n",
       "        1.82640145e-01, -9.16783275e-02, -5.21694779e-01,  1.21994545e-01,\n",
       "       -1.90354214e-01,  3.45929609e-01, -6.39620215e-02,  6.98215836e-01,\n",
       "        6.50689611e-02, -2.29641814e-02,  1.57412351e-02, -8.39547702e-03,\n",
       "        3.20811310e-01,  4.98331535e-01,  1.98689248e-01,  2.81684333e-01,\n",
       "        2.56380488e-01,  3.00457887e-01, -2.23391219e-04,  5.88743975e-01,\n",
       "        1.70543787e-01,  5.15946416e-01,  5.28951363e-01,  5.47468505e-03,\n",
       "       -2.21889069e-01, -7.27561108e-02,  3.66049847e-01, -4.51141840e-02,\n",
       "       -1.67830212e-01,  2.74762161e-01, -1.30383492e-01,  2.56454542e-02,\n",
       "       -4.30234307e-02, -5.32909834e-01, -9.04343319e-02,  4.62973046e-01,\n",
       "        6.16148618e-02,  1.09103519e-01, -4.56836385e-01, -2.21006493e-01,\n",
       "       -8.15083597e-02, -1.54733535e-01, -3.45846559e-01,  3.08563588e-02,\n",
       "        3.18947413e-01,  4.61782153e-01,  1.23670578e-01,  5.90845192e-01,\n",
       "       -1.36442415e-02, -5.93859958e-01, -3.30015528e-01, -4.84634855e-01,\n",
       "       -4.26163972e-02, -2.53200272e-02,  2.06369373e-01, -2.33514032e-01,\n",
       "        8.61472220e-02, -8.66436620e-02,  6.01603593e-01, -5.58487574e-01,\n",
       "       -1.78398254e-02, -1.10052788e-02,  5.60040596e-01, -2.44888920e-02,\n",
       "       -8.21199073e-02,  5.38188724e-01, -3.49379308e-02,  2.14550225e-02,\n",
       "        4.85425596e-03, -1.05072657e-01, -1.06553366e-01, -1.21741545e-02,\n",
       "        1.06659458e-01, -3.37372313e-01,  2.54706120e-01,  6.84195857e-02,\n",
       "       -1.68724311e-02,  1.51583551e-02,  9.76389764e-02, -1.39955654e-01,\n",
       "        6.35923028e-01,  1.43219753e-01,  3.49290475e-02,  3.15655085e-01,\n",
       "        5.54411607e-02, -1.03165703e-01,  2.28838521e-01,  2.37299339e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_me(a):\n",
    "    pos_above_iter = 0\n",
    "\n",
    "    for y, pred in a:\n",
    "        if y == 0:\n",
    "            if (a[a[:, 1]==pred]).shape[0] > 1:\n",
    "                pos_above_iter += np.sum(a[a[:,1] > pred][:, 0], axis=0) / 2\n",
    "            pos_above_iter += np.sum(a[a[:,1] > pred][:, 0], axis=0)\n",
    "    return pos_above_iter / (np.sum(a[:,0]==1) * np.sum(a[:,0]==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_me(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6944444444444444"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwe = np.array([[1, 0.91],\n",
    "                [0, 0.86],\n",
    "                [0, 0.78],\n",
    "                [1, 0.6],\n",
    "                [0, 0.6],\n",
    "                [1, 0.55],\n",
    "                [0, 0.51],\n",
    "                [0, 0.46],\n",
    "                [0, 0.42],])\n",
    "\n",
    "\n",
    "try_me(qwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lambda x, y: -np.mean(np.log(x)*y + np.log(1 - x)*(1-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(np.array([0.001, 0.001, 0.001, 0.999, 0.999]).T)\n",
    "y = pd.Series(np.array([0, 0, 0, 1, 1]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([1, 0, 1, 1, 0])\n",
    "true = np.array([1, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(pred == true) / (np.sum(pred == true) + np.sum((pred == 1) & (true == 0)))\n",
    "q = lambda x, y : x * (y> 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(5, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Пример предсказанных и истинных меток классов\n",
    "predicted_labels = [1, 0, 1, 1, 0]\n",
    "true_labels = [1, 1, 1, 0, 1]\n",
    "\n",
    "# Расчет precision\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x():\n",
    "    print('1231213')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Версия без mini_batch\n",
    "\n",
    "# class MyLogReg():\n",
    "#     def __init__(self, n_iter, learning_rate, weights=None, metric=None, reg=None, l1_coef=0, l2_coef=0):\n",
    "#         self.n_iter = n_iter\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.weights = weights\n",
    "#         self.metric = metric\n",
    "#         self.reg = reg\n",
    "#         self.l1_coef = l1_coef\n",
    "#         self.l2_coef = l2_coef\n",
    "\n",
    "#         self.metric = self.metrics(metric)\n",
    "#         self.learning_rate = self.learning_rate_type(learning_rate)\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return f\"MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\"\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return f\"MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\"\n",
    "\n",
    "#     def calculate_gradient(self, X, y, y_pred):\n",
    "#         if self.reg == 'l1':\n",
    "#             assert self.l1_coef != 0\n",
    "#             return (1/X.shape[0]) * np.dot((y_pred - y.values.ravel()), X) + self.l1_coef*np.sign(self.weights)\n",
    "#         elif self.reg == 'l2':\n",
    "#             assert self.l2_coef != 0\n",
    "#             return (1/X.shape[0]) * np.dot((y_pred - y.values.ravel()), X) + self.l2_coef*2*(self.weights)\n",
    "#         elif self.reg == 'elasticnet':\n",
    "#             assert self.l1_coef != 0 and self.l2_coef != 0\n",
    "#             return (1/X.shape[0]) * np.dot((y_pred - y.values.ravel()), X) + self.l1_coef*np.sign(self.weights) + self.l2_coef*2*(self.weights)\n",
    "#         else:\n",
    "#             return (1/X.shape[0]) * np.dot((y_pred - y.values.ravel()), X)\n",
    "\n",
    "#     def learning_rate_type(self, LR):\n",
    "#         if not isinstance(LR, float):\n",
    "#             return LR\n",
    "#         else:\n",
    "#             return lambda x: LR\n",
    "\n",
    "#     def metrics(self, metric) -> None:\n",
    "#         if metric:\n",
    "#             if metric == 'accuracy':\n",
    "#                 metric = ['accuracy', lambda y, y_pred: (\n",
    "#                     np.sum(y.values.ravel() == (y_pred > 0.5).ravel())) / len(y)]\n",
    "\n",
    "#             elif metric == 'precision':\n",
    "#                 # TP / (TP + FP)\n",
    "#                 def precision_score_func(y, y_pred):\n",
    "#                     y_pred_binary = (y_pred > 0.5).ravel()\n",
    "#                     y_ravel = y.values.ravel()\n",
    "#                     TP = np.sum((y_ravel == 1) & (y_pred_binary == 1))\n",
    "#                     FP = np.sum((y_ravel == 0) & (y_pred_binary == 1))\n",
    "#                     return TP/(TP+FP)\n",
    "\n",
    "#                 metric = ['precision', precision_score_func]\n",
    "\n",
    "#             elif metric == 'recall':\n",
    "#                 # TP / (TP + FN)\n",
    "#                 def recall_score_func(y, y_pred):\n",
    "#                     y_pred_binary = (y_pred > 0.5)\n",
    "#                     y_ravel = y.values.ravel()\n",
    "#                     TP = np.sum((y_ravel == 1) & (y_pred_binary == 1))\n",
    "#                     FN = np.sum((y_ravel == 1) & (y_pred_binary == 0))\n",
    "#                     return TP/(TP+FN)\n",
    "                \n",
    "#                 metric = ['recall', recall_score_func]\n",
    "\n",
    "#             elif metric == 'f1':\n",
    "#                 # 2 * precision * recall / (precision + recall)\n",
    "#                 def f1_score_func(y, y_pred):\n",
    "#                     y_pred_binary = (y_pred > 0.5)\n",
    "#                     y_ravel = y.values.ravel()\n",
    "#                     TP = np.sum((y_ravel == 1) & (y_pred_binary == 1))\n",
    "#                     FP = np.sum((y_ravel == 0) & (y_pred_binary == 1))\n",
    "#                     FN = np.sum((y_ravel == 1) & (y_pred_binary == 0))\n",
    "#                     precision = TP/(TP+FP)\n",
    "#                     recall = TP/(TP+FN)\n",
    "#                     return 2 * precision * recall / (precision + recall)\n",
    "                \n",
    "#                 metric = ['f1', f1_score_func]\n",
    "\n",
    "\n",
    "#             elif metric == 'roc_auc':\n",
    "#                 def auc_score_def(y, y_pred):\n",
    "#                     data = np.concatenate(\n",
    "#                         (y.to_numpy().reshape(-1, 1), np.round(y_pred.reshape(-1, 1), 10)), axis=1)\n",
    "#                     data = data[data[:, 1].argsort()][::-1]\n",
    "\n",
    "#                     pos_above_iter = 0\n",
    "\n",
    "#                     for y, pred in data:\n",
    "#                         if y == 0:\n",
    "#                             if (data[data[:, 1] == pred]).shape[0] > 1:\n",
    "#                                 pos_above_iter += np.sum(\n",
    "#                                     data[data[:, 1] > pred][:, 0], axis=0) / 2\n",
    "#                             pos_above_iter += np.sum(\n",
    "#                                 data[data[:, 1] > pred][:, 0], axis=0)\n",
    "#                     return pos_above_iter / (np.sum(data[:, 0] == 1) * np.sum(data[:, 0] == 0))\n",
    "\n",
    "#                 metric = ['roc_auc', auc_score_def]\n",
    "\n",
    "#         return metric\n",
    "\n",
    "#     def fit(self, samples: pd.DataFrame, y: pd.Series, verbose=False) -> None:\n",
    "#         X = samples.copy()\n",
    "#         X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "\n",
    "#         self.weights = np.ones(X.shape[1])\n",
    "\n",
    "#         for iter in range(1, self.n_iter+1):\n",
    "\n",
    "#             y_pred = np.array(1/(1 + np.exp(-np.dot(X, self.weights))))\n",
    "\n",
    "#             loss = -np.mean(np.log(y_pred+1e-100)*y.values.ravel() +\n",
    "#                             np.log(1 - y_pred+1e-100)*(1-y.values.ravel()))\n",
    "\n",
    "#             # grad = (1/(X.shape[0]) * np.dot((y_pred - y.values.ravel()), X))\n",
    "#             grad = self.calculate_gradient(X, y, y_pred)\n",
    "\n",
    "#             self.weights = self.weights - grad * self.learning_rate(iter)\n",
    "#             if verbose and (iter % verbose) == 0 and self.metric is not None:\n",
    "#                 print(\n",
    "#                     f'iter = {iter+1} ||| Loss = {loss} ||| {self.metric[0]} = {self.metric[1](y, y_pred)}')\n",
    "#             elif verbose and (iter % verbose) == 0:\n",
    "#                 print(f'iter = {iter+1} ||| Loss = {loss}')\n",
    "#         if self.metric:\n",
    "#             self.final_metric = self.metric[1](y, np.array(\n",
    "#                 1/(1 + np.exp(-np.dot(X, self.weights)))))\n",
    "\n",
    "#     def predict(self, samples):\n",
    "#         X = samples.copy()\n",
    "#         X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "#         return (np.array(1/(1 + np.exp(-np.dot(X, self.weights)))) > 0.5).astype(np.int8)\n",
    "\n",
    "#     def predict_proba(self, samples):\n",
    "#         X = samples.copy()\n",
    "#         X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "#         return np.array(1/(1 + np.exp(-np.dot(X, self.weights))))\n",
    "\n",
    "#     def get_coef(self) -> list():\n",
    "#         try:\n",
    "#             assert self.weights is not None\n",
    "#             return np.array(self.weights[1:])\n",
    "#         except:\n",
    "#             return 'fit before!'\n",
    "\n",
    "#     def get_best_score(self) -> int:\n",
    "#         return self.final_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "class MyLogReg():\n",
    "    def __init__(self, n_iter, learning_rate, weights=None, sgd_sample=None, random_state=42):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = weights\n",
    "        self.random_state = random_state\n",
    "        self.sgd_sample = sgd_sample\n",
    "\n",
    "        self.learning_rate = self.learning_rate_type(learning_rate)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\"\n",
    "\n",
    "\n",
    "    def learning_rate_type(self, LR):\n",
    "        if not isinstance(LR, float):\n",
    "            return LR\n",
    "        else:\n",
    "            return lambda x: LR\n",
    "        \n",
    "    def sgd_sample_size(self, sgd_sample_num, samples):\n",
    "        if isinstance(sgd_sample_num, float):\n",
    "            return int(samples * sgd_sample_num)\n",
    "        elif isinstance(sgd_sample_num, int):\n",
    "            return sgd_sample_num\n",
    "        else:\n",
    "            return samples\n",
    "\n",
    "\n",
    "    def fit(self, samples: pd.DataFrame, y: pd.Series, verbose=False) -> None:\n",
    "        random.seed(self.random_state)  # фиксируем рандом сид\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        sgd_sample_quantity = self.sgd_sample_size(self.sgd_sample, samples.shape[0])\n",
    "\n",
    "        X = samples.copy()\n",
    "        X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "\n",
    "        self.weights = np.ones(X.shape[1])\n",
    "\n",
    "        for iter in range(1, self.n_iter+1):\n",
    "            sample_rows_idx = random.sample(\n",
    "                range(X.shape[0]), sgd_sample_quantity)\n",
    "            X_mini_batch = X.iloc[sample_rows_idx]\n",
    "            y_mini_batch = y.iloc[sample_rows_idx]\n",
    "\n",
    "            y_pred = np.array(1/(1 + np.exp(-np.dot(X_mini_batch, self.weights))))\n",
    "\n",
    "            grad = (1/(X.shape[0]) * np.dot((y_pred - y_mini_batch.values.ravel()), X_mini_batch))\n",
    "\n",
    "            # grad = self.calculate_gradient(X_mini_batch, y_mini_batch, y_pred)\n",
    "            self.weights = self.weights - grad * self.learning_rate(iter)\n",
    "\n",
    "\n",
    "    def predict(self, samples):\n",
    "        X = samples.copy()\n",
    "        X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "        return (np.array(1/(1 + np.exp(-np.dot(X, self.weights)))) > 0.5).astype(np.int8)\n",
    "\n",
    "    def predict_proba(self, samples):\n",
    "        X = samples.copy()\n",
    "        X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "        return np.array(1/(1 + np.exp(-np.dot(X, self.weights))))\n",
    "\n",
    "    def get_coef(self) -> list():\n",
    "        try:\n",
    "            assert self.weights is not None\n",
    "            return np.array(self.weights[1:])\n",
    "        except:\n",
    "            return 'fit before!'\n",
    "\n",
    "    def get_best_score(self) -> int:\n",
    "        return self.final_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n",
      "(40,) (40, 101)\n",
      "(101,)\n"
     ]
    }
   ],
   "source": [
    "x = MyLogReg(50, 0.1, sgd_sample=0.1)\n",
    "np.random.seed(42)\n",
    "\n",
    "# X = pd.concat((X_1, X_2), axis=0)\n",
    "X = pd.DataFrame(np.random.randint(-25, 25, (400, 100)))\n",
    "y = pd.DataFrame({'target': [0 if x<200 else 1 for x in range(400)]})\n",
    "\n",
    "x.fit(X, y, verbose=10)\n",
    "\n",
    "# np.mean((x.predict_proba(X) > 0.5) == y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
