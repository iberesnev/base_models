{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MyLogReg():\n",
    "    def __init__(self, n_iter, learning_rate, weights=None, metric=None):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "\n",
    "        self.metric = self.metrics(metric)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\"\n",
    "\n",
    "    def metrics(self, metric) -> None:\n",
    "        if metric:\n",
    "            if metric == 'accuracy':\n",
    "                metric = ['accuracy', lambda y, y_pred: (\n",
    "                    np.sum(y.values.ravel() == (y_pred > 0.5).ravel())) / len(y)]\n",
    "\n",
    "            elif metric == 'precision':\n",
    "                # TP / (TP + FP)\n",
    "                def precision_score_func(y, y_pred):\n",
    "                    y_pred_binary = (y_pred > 0.5).ravel()\n",
    "                    y_ravel = y.values.ravel()\n",
    "                    TP = np.sum((y_ravel == 1) & (y_pred_binary == 1))\n",
    "                    FP = np.sum((y_ravel == 0) & (y_pred_binary == 1))\n",
    "                    return TP/(TP+FP)\n",
    "\n",
    "                metric = ['precision', precision_score_func]\n",
    "\n",
    "            elif metric == 'recall':\n",
    "                # TP / (TP + FN)\n",
    "                def recall_score_func(y, y_pred):\n",
    "                    y_pred_binary = (y_pred > 0.5)\n",
    "                    y_ravel = y.values.ravel()\n",
    "                    TP = np.sum((y_ravel == 1) & (y_pred_binary == 1))\n",
    "                    FN = np.sum((y_ravel == 1) & (y_pred_binary == 0))\n",
    "                    return TP/(TP+FN)\n",
    "                \n",
    "                metric = ['recall', recall_score_func]\n",
    "\n",
    "            elif metric == 'f1':\n",
    "                # 2 * precision * recall / (precision + recall)\n",
    "                def f1_score_func(y, y_pred):\n",
    "                    y_pred_binary = (y_pred > 0.5)\n",
    "                    y_ravel = y.values.ravel()\n",
    "                    TP = np.sum((y_ravel == 1) & (y_pred_binary == 1))\n",
    "                    FP = np.sum((y_ravel == 0) & (y_pred_binary == 1))\n",
    "                    FN = np.sum((y_ravel == 1) & (y_pred_binary == 0))\n",
    "                    precision = TP/(TP+FP)\n",
    "                    recall = TP/(TP+FN)\n",
    "                    return 2 * precision * recall / (precision + recall)\n",
    "                \n",
    "                metric = ['f1', f1_score_func]\n",
    "\n",
    "\n",
    "            elif metric == 'roc_auc':\n",
    "                def auc_score_def(y, y_pred):\n",
    "                    data = np.concatenate(\n",
    "                        (y.to_numpy().reshape(-1, 1), np.round(y_pred.reshape(-1, 1), 10)), axis=1)\n",
    "                    data = data[data[:, 1].argsort()][::-1]\n",
    "\n",
    "                    pos_above_iter = 0\n",
    "\n",
    "                    for y, pred in data:\n",
    "                        if y == 0:\n",
    "                            if (data[data[:, 1] == pred]).shape[0] > 1:\n",
    "                                pos_above_iter += np.sum(\n",
    "                                    data[data[:, 1] > pred][:, 0], axis=0) / 2\n",
    "                            pos_above_iter += np.sum(\n",
    "                                data[data[:, 1] > pred][:, 0], axis=0)\n",
    "                    return pos_above_iter / (np.sum(data[:, 0] == 1) * np.sum(data[:, 0] == 0))\n",
    "\n",
    "                metric = ['roc_auc', auc_score_def]\n",
    "\n",
    "        return metric\n",
    "\n",
    "    def fit(self, samples: pd.DataFrame, y: pd.Series, verbose=False) -> None:\n",
    "        X = samples.copy()\n",
    "        X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "\n",
    "        self.weights = np.ones(X.shape[1])\n",
    "\n",
    "        for iter in range(1, self.n_iter+1):\n",
    "\n",
    "            y_pred = np.array(1/(1 + np.exp(-np.dot(X, self.weights))))\n",
    "\n",
    "            loss = -np.mean(np.log(y_pred+1e-100)*y.values.ravel() +\n",
    "                            np.log(1 - y_pred+1e-100)*(1-y.values.ravel()))\n",
    "\n",
    "            grad = (1/(X.shape[0]) * np.dot((y_pred - y.values.ravel()), X))\n",
    "\n",
    "            self.weights = self.weights - grad * self.learning_rate\n",
    "            if verbose and (iter % verbose) == 0 and self.metric is not None:\n",
    "                print(\n",
    "                    f'iter = {iter+1} ||| Loss = {loss} ||| {self.metric[0]} = {self.metric[1](y, y_pred)}')\n",
    "            elif verbose and (iter % verbose) == 0:\n",
    "                print(f'iter = {iter+1} ||| Loss = {loss}')\n",
    "        if self.metric:\n",
    "            self.final_metric = self.metric[1](y, np.array(\n",
    "                1/(1 + np.exp(-np.dot(X, self.weights)))))\n",
    "\n",
    "    def predict(self, samples):\n",
    "        X = samples.copy()\n",
    "        X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "        return (np.array(1/(1 + np.exp(-np.dot(X, self.weights)))) > 0.5).astype(np.int8)\n",
    "\n",
    "    def predict_proba(self, samples):\n",
    "        X = samples.copy()\n",
    "        X.insert(0, 'bias', pd.Series(1, index=range(X.shape[0])))\n",
    "        return np.array(1/(1 + np.exp(-np.dot(X, self.weights))))\n",
    "\n",
    "    def get_coef(self) -> list():\n",
    "        try:\n",
    "            assert self.weights is not None\n",
    "            return np.array(self.weights[1:])\n",
    "        except:\n",
    "            return 'fit before!'\n",
    "\n",
    "    def get_best_score(self) -> int:\n",
    "        return self.final_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = MyLogReg(50, 0.1, metric='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 11 ||| Loss = 29.54050128760057 ||| recall = 0.48\n",
      "iter = 21 ||| Loss = 7.416131691705216 ||| recall = 0.635\n",
      "iter = 31 ||| Loss = 2.1221107358506774 ||| recall = 0.715\n",
      "iter = 41 ||| Loss = 2.3451749835523397 ||| recall = 0.715\n",
      "iter = 51 ||| Loss = 2.373551850025176 ||| recall = 0.695\n"
     ]
    }
   ],
   "source": [
    "# X_1 = pd.DataFrame({'first':[50 for x in range(200)], 'second':[0 for x in range(200)]})\n",
    "# X_2 = pd.DataFrame({'first':[0 for x in range(200)], 'second':[50 for x in range(200)]})\n",
    "\n",
    "# X = pd.concat((X_1, X_2), axis=0)\n",
    "X = pd.DataFrame(np.random.randint(-25, 25, (400, 100)))\n",
    "y = pd.DataFrame({'target': [0 if x<200 else 1 for x in range(400)]})\n",
    "\n",
    "x.fit(X, y, verbose=10)\n",
    "\n",
    "# np.mean((x.predict_proba(X) > 0.5) == y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.concatenate((y.to_numpy().reshape(400, 1), x.predict_proba(X).reshape(400, 1)), axis=1)\n",
    "# a = a[a[:, 1].argsort()][::-1]\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_me(a):\n",
    "    pos_above_iter = 0\n",
    "\n",
    "    for y, pred in a:\n",
    "        if y == 0:\n",
    "            if (a[a[:, 1]==pred]).shape[0] > 1:\n",
    "                pos_above_iter += np.sum(a[a[:,1] > pred][:, 0], axis=0) / 2\n",
    "            pos_above_iter += np.sum(a[a[:,1] > pred][:, 0], axis=0)\n",
    "    return pos_above_iter / (np.sum(a[:,0]==1) * np.sum(a[:,0]==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_me(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6944444444444444"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwe = np.array([[1, 0.91],\n",
    "                [0, 0.86],\n",
    "                [0, 0.78],\n",
    "                [1, 0.6],\n",
    "                [0, 0.6],\n",
    "                [1, 0.55],\n",
    "                [0, 0.51],\n",
    "                [0, 0.46],\n",
    "                [0, 0.42],])\n",
    "\n",
    "\n",
    "try_me(qwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lambda x, y: -np.mean(np.log(x)*y + np.log(1 - x)*(1-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(np.array([0.001, 0.001, 0.001, 0.999, 0.999]).T)\n",
    "y = pd.Series(np.array([0, 0, 0, 1, 1]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([1, 0, 1, 1, 0])\n",
    "true = np.array([1, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(pred == true) / (np.sum(pred == true) + np.sum((pred == 1) & (true == 0)))\n",
    "q = lambda x, y : x * (y> 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(5, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Пример предсказанных и истинных меток классов\n",
    "predicted_labels = [1, 0, 1, 1, 0]\n",
    "true_labels = [1, 1, 1, 0, 1]\n",
    "\n",
    "# Расчет precision\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x():\n",
    "    print('1231213')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ('mse', x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
